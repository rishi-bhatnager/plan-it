{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Practice\n",
    "Learning how to implement an RL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *The taxi problem using OpenAI's Gym:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : :\u001b[43m \u001b[0m|\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  500  = number of possible states (25 spaces on board x 4 destinations x (4+1) passenger locations)\n",
      "\n",
      "First 3 entries:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{0: [(1.0, 100, -1, False)],\n",
       "  1: [(1.0, 0, -1, False)],\n",
       "  2: [(1.0, 20, -1, False)],\n",
       "  3: [(1.0, 0, -1, False)],\n",
       "  4: [(1.0, 16, -1, False)],\n",
       "  5: [(1.0, 0, -10, False)]},\n",
       " {0: [(1.0, 101, -1, False)],\n",
       "  1: [(1.0, 1, -1, False)],\n",
       "  2: [(1.0, 21, -1, False)],\n",
       "  3: [(1.0, 1, -1, False)],\n",
       "  4: [(1.0, 17, -1, False)],\n",
       "  5: [(1.0, 1, -10, False)]},\n",
       " {0: [(1.0, 102, -1, False)],\n",
       "  1: [(1.0, 2, -1, False)],\n",
       "  2: [(1.0, 22, -1, False)],\n",
       "  3: [(1.0, 2, -1, False)],\n",
       "  4: [(1.0, 18, -1, False)],\n",
       "  5: [(1.0, 2, -10, False)]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rewards table\n",
    "print('Length: ', len(env.P), ' = number of possible states (25 spaces on board x 4 destinations x (4+1) passenger locations)')\n",
    "#                                                                                                   ^ bc R,G,Y,B or in taxi\n",
    "print('\\nFirst 3 entries:')\n",
    "[env.P[i] for i in range(3)]\n",
    "\n",
    "# Structure of each entry: {action: [(probability, nextstate, reward, done)]}\n",
    "#    6 possible actions: move N/S/W/E, pick up, drop off\n",
    "#    done = whether a passenger was successfully dropped off in the right location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D array that initializes all rewards to 0\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "alpha = .1\n",
    "gamma = .6\n",
    "epsilon = .1\n",
    "\n",
    "all_epochs = []\n",
    "all_penalties = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c39a12ad7ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training finished.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_penalties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'green'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Penalties per Episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "# the RL (Q-Learning) model\n",
    "# animation:\n",
    "#    - R,G,Y,B = destinations (blue = passenger for pickup, purple = destination for dropoff)\n",
    "#    - yellow block = empty taxi, green block = full taxi\n",
    "#    - pauses after each episode (which is 1 successful pickup and dropoff)\n",
    "for i in range(1,1001):\n",
    "    state = env.reset()\n",
    "    \n",
    "    epochs, penalties, reward = 0,0,0\n",
    "    done = False\n",
    "    frames = []\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            # Exploration (try some random action)\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            # Exploitation (use best known action, or \"exploit\" current knowledge)\n",
    "            # Note: indexing the q_table just by the state gives all possible actions for that state,\n",
    "            #   and then argmax gives the action with the highest reward\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "        # Take the action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        old_value = q_table[state,action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        # Update Q-value\n",
    "        q_table[state,action] = (1-alpha)*old_value + alpha*(reward + gamma*next_max)\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "            \n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "        })\n",
    "            \n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    all_penalties.append(penalties)\n",
    "    if i % 100 == 0:\n",
    "        sleep(1 if i != 100 else 0)\n",
    "        clear_output(wait=True)\n",
    "        print(f'Episode: {i}')\n",
    "        sleep(1)\n",
    "        \n",
    "        for i, frame in enumerate(frames):\n",
    "            clear_output(wait=True)\n",
    "            print(frame['frame'])\n",
    "            print(f\"Timestep: {i + 1}\")\n",
    "            print(f\"State: {frame['state']}\")\n",
    "            print(f\"Action: {frame['action']}\")\n",
    "            print(f\"Reward: {frame['reward']}\")\n",
    "            sleep(.1)\n",
    "\n",
    "\n",
    "sleep(1)\n",
    "clear_output(wait=True)\n",
    "print('Training finished.')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(all_penalties, color='green')\n",
    "plt.title('Penalties per Episode')\n",
    "plt.ylabel('Number of Penalties')\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PPO2' from 'stable_baselines' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ef85fe2d6d0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPPO2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m model = DQN('MlpPolicy', 'Taxi-v3', gamma=.6, learning_rate=.1, exploration_initial_eps=.05, \n\u001b[1;32m      3\u001b[0m             exploration_final_eps=.15, verbose=1)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreward_before_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PPO2' from 'stable_baselines' (unknown location)"
     ]
    }
   ],
   "source": [
    "from stable_baselines import DQN\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "env2 = gym.make(\"Taxi-v3\")\n",
    "model = DQN('MlpPolicy', env2, gamma=.6, learning_rate=.1, exploration_initial_eps=.05, \n",
    "            exploration_final_eps=.15, verbose=1)\n",
    "\n",
    "mean_reward_before_train, _ = evaluate_policy(model, env2, n_eval_episodes=1000)\n",
    "mean_reward_before_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10000, log_interval=10)\n",
    "mean_reward, _ = evaluate_policy(model, env2, n_eval_episodes=1000)\n",
    "mean_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stable_baselines",
   "language": "python",
   "name": "stable_baselines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
